{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55c940c1",
      "metadata": {
        "id": "55c940c1"
      },
      "source": [
        "\n",
        "# ✅ 多模态 Transformer（IMU + TOF） + 时域/频域（FFT & 小波）版本\n",
        "本 Notebook 已将原来的 BERT 方案**替换/禁用**，并内嵌：\n",
        "- **FFT + Wavelet** 频域特征（以额外 token 注入 Transformer）\n",
        "- **IMU + TOF 多模态 Transformer 融合模型**\n",
        "- **一键可跑的脚本**：自动检测你是否已经有数据管道；若没有则使用演示数据跑通\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "602e1050",
      "metadata": {
        "id": "602e1050"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, gc, time, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    import pywt\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"需要 PyWavelets，请先安装：!pip install PyWavelets\") from e\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0557c970",
      "metadata": {
        "id": "0557c970"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ensure_3d(x):\n",
        "    x = np.asarray(x)\n",
        "    if x.ndim == 2: x = x[None, ...]\n",
        "    if x.ndim != 3: raise ValueError(f\"Expect [B,T,C], got {x.shape}\")\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "def fft_token_from_sequence(x_np: np.ndarray, top_k: int = 32) -> np.ndarray:\n",
        "    x_np = ensure_3d(x_np)  # [B,T,C]\n",
        "    mag = np.abs(np.fft.rfft(x_np, axis=1))  # [B, T_fft, C]\n",
        "    k = min(top_k, mag.shape[1])\n",
        "    token = np.log1p(mag[:, :k, :]).reshape(x_np.shape[0], -1).astype(np.float32)\n",
        "    return token  # [B, k*C]\n",
        "\n",
        "def wavelet_token_from_sequence(x_np: np.ndarray, wavelet: str = \"db4\", level: int = 3) -> np.ndarray:\n",
        "    x_np = ensure_3d(x_np)\n",
        "    B, T, C = x_np.shape\n",
        "    feats = []\n",
        "    for b in range(B):\n",
        "        ch = []\n",
        "        for c in range(C):\n",
        "            coeffs = pywt.wavedec(x_np[b, :, c], wavelet=wavelet, level=level, mode='symmetric')\n",
        "            for arr in coeffs:\n",
        "                ch.append(float(np.mean(arr)))\n",
        "                ch.append(float(np.std(arr)))\n",
        "        feats.append(ch)\n",
        "    return np.asarray(feats, dtype=np.float32)  # [B, 2*(level+1)*C]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2342a2c4",
      "metadata": {
        "id": "2342a2c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TokenBuilder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, d_model)\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model))\n",
        "        self.ffn_fft = nn.Linear(1, d_model)\n",
        "        self.ffn_wav = nn.Linear(1, d_model)\n",
        "        self.pos_embed = None\n",
        "\n",
        "    def add_positional_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pos_embed = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x, fft_token_np, wav_token_np):\n",
        "        B, T, _ = x.shape\n",
        "        h = self.proj(x)\n",
        "        if self.pos_embed is not None and self.pos_embed.shape[1] >= T:\n",
        "            h = h + self.pos_embed[:, :T, :].to(h.device)\n",
        "        # Pool freq tokens -> scalar then project\n",
        "        fft_pool = torch.from_numpy(fft_token_np).to(x.device).mean(dim=1, keepdim=True).unsqueeze(-1)  # [B,1,1]\n",
        "        wav_pool = torch.from_numpy(wav_token_np).to(x.device).mean(dim=1, keepdim=True).unsqueeze(-1)  # [B,1,1]\n",
        "        fft_tok = self.ffn_fft(fft_pool)\n",
        "        wav_tok = self.ffn_wav(wav_pool)\n",
        "        cls_tok = self.cls.expand(B, -1, -1)\n",
        "        return torch.cat([cls_tok, h, fft_tok, wav_tok], dim=1)\n",
        "\n",
        "class MultiModalTransformer(nn.Module):\n",
        "    def __init__(self, imu_dim, tof_dim, d_model=128, nhead=4, num_layers=2, num_classes=18, max_len=512):\n",
        "        super().__init__()\n",
        "        self.imu_tokens = TokenBuilder(imu_dim, d_model)\n",
        "        self.tof_tokens = TokenBuilder(tof_dim, d_model)\n",
        "        self.imu_tokens.add_positional_encoding(max_len, d_model)\n",
        "        self.tof_tokens.add_positional_encoding(max_len, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, dim_feedforward=4*d_model)\n",
        "        self.enc_imu = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.enc_tof = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2*d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, imu_x, tof_x, imu_fft_tok, imu_wav_tok, tof_fft_tok, tof_wav_tok):\n",
        "        s_imu = self.imu_tokens(imu_x, imu_fft_tok, imu_wav_tok)\n",
        "        s_tof = self.tof_tokens(tof_x, tof_fft_tok, tof_wav_tok)\n",
        "        h_imu = self.enc_imu(s_imu)[:, 0, :]\n",
        "        h_tof = self.enc_tof(s_tof)[:, 0, :]\n",
        "        fused = torch.cat([h_imu, h_tof], dim=-1)\n",
        "        return self.classifier(fused)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f990e1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f990e1a",
        "outputId": "5c235f8e-fdb8-49cf-9147-a0f33ff2c394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=2.9866, acc=0.1094\n",
            "Epoch 2: loss=2.9349, acc=0.0781\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Auto-detect existing pipeline or run a demo ---\n",
        "# EXPECTED (if you already have data):\n",
        "#   - imu_train: np.ndarray [N, T, C_imu]\n",
        "#   - tof_train: np.ndarray [N, T, C_tof]\n",
        "#   - y_train:   np.ndarray [N] or one-hot [N, num_classes]\n",
        "#\n",
        "# If not found, we'll create a small synthetic dataset and run end-to-end.\n",
        "\n",
        "def has_existing_vars(globals_d):\n",
        "    needed = ['imu_train', 'tof_train', 'y_train']\n",
        "    return all(k in globals_d for k in needed)\n",
        "\n",
        "class SimpleTensorDataset(Dataset):\n",
        "    def __init__(self, imu_np, tof_np, y_np, num_classes):\n",
        "        self.imu = imu_np.astype(np.float32)\n",
        "        self.tof = tof_np.astype(np.float32)\n",
        "        self.y = y_np.astype(np.int64) if y_np.ndim==1 else np.argmax(y_np, axis=1).astype(np.int64)\n",
        "        self.num_classes = num_classes\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        imu_x = self.imu[idx]  # [T,C_imu]\n",
        "        tof_x = self.tof[idx]  # [T,C_tof]\n",
        "        y = self.y[idx]\n",
        "        return imu_x, tof_x, y\n",
        "\n",
        "def collate_with_freq(batch):\n",
        "    imu_list, tof_list, y_list = zip(*batch)\n",
        "    imu_np = np.stack(imu_list)  # [B,T,C_imu]\n",
        "    tof_np = np.stack(tof_list)  # [B,T,C_tof]\n",
        "    # freq tokens\n",
        "    imu_fft = fft_token_from_sequence(imu_np, top_k=32)\n",
        "    imu_wav = wavelet_token_from_sequence(imu_np, level=3)\n",
        "    tof_fft = fft_token_from_sequence(tof_np, top_k=16)\n",
        "    tof_wav = wavelet_token_from_sequence(tof_np, level=3)\n",
        "    # tensors\n",
        "    imu_x = torch.from_numpy(imu_np)\n",
        "    tof_x = torch.from_numpy(tof_np)\n",
        "    y = torch.tensor(np.array(y_list), dtype=torch.long)\n",
        "    return imu_x, tof_x, y, imu_fft, imu_wav, tof_fft, tof_wav\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for imu_x, tof_x, y, imu_fft, imu_wav, tof_fft, tof_wav in loader:\n",
        "        imu_x, tof_x, y = imu_x.to(device), tof_x.to(device), y.to(device)\n",
        "        logits = model(imu_x, tof_x, imu_fft, imu_wav, tof_fft, tof_wav)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        total += y.size(0)\n",
        "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "# Prepare data\n",
        "if has_existing_vars(globals()):\n",
        "    imu_np, tof_np, y_np = imu_train, tof_train, y_train\n",
        "    num_classes = int(np.max(y_np))+1 if y_np.ndim==1 else y_np.shape[1]\n",
        "else:\n",
        "    # Demo synthetic data\n",
        "    N, T, C_imu, C_tof, num_classes = 128, 120, 6, 64, 18\n",
        "    imu_np = np.random.randn(N, T, C_imu).astype(np.float32)*0.5\n",
        "    tof_np = np.random.randn(N, T, C_tof).astype(np.float32)*0.2\n",
        "    y_np = np.random.randint(0, num_classes, size=(N,))\n",
        "\n",
        "dataset = SimpleTensorDataset(imu_np, tof_np, y_np, num_classes)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_with_freq)\n",
        "\n",
        "model = MultiModalTransformer(imu_dim=imu_np.shape[2], tof_dim=tof_np.shape[2],\n",
        "                              d_model=128, nhead=4, num_layers=2,\n",
        "                              num_classes=num_classes, max_len=imu_np.shape[1]+3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train a couple of epochs to verify it's runnable\n",
        "for epoch in range(2):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}: loss={tr_loss:.4f}, acc={tr_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6488f2",
      "metadata": {
        "papermill": {
          "duration": 0.003949,
          "end_time": "2025-07-22T14:39:00.945710",
          "exception": false,
          "start_time": "2025-07-22T14:39:00.941761",
          "status": "completed"
        },
        "tags": [],
        "id": "db6488f2"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fafd8c1b",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-07-22T14:39:00.953301Z",
          "iopub.status.busy": "2025-07-22T14:39:00.953016Z",
          "iopub.status.idle": "2025-07-22T14:39:33.273240Z",
          "shell.execute_reply": "2025-07-22T14:39:33.272613Z"
        },
        "papermill": {
          "duration": 32.325626,
          "end_time": "2025-07-22T14:39:33.274803",
          "exception": false,
          "start_time": "2025-07-22T14:39:00.949177",
          "status": "completed"
        },
        "tags": [],
        "id": "fafd8c1b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.amp import autocast\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from transformers import BertConfig, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27370106",
      "metadata": {
        "papermill": {
          "duration": 0.003103,
          "end_time": "2025-07-22T14:39:33.281620",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.278517",
          "status": "completed"
        },
        "tags": [],
        "id": "27370106"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Use tof raw data and split tof statistic data, gap=16 is best in my trials.\n",
        "\n",
        "基于物理公式从 IMU 提取特征"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f2b6e2a9",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2025-07-22T14:39:33.289208Z",
          "iopub.status.busy": "2025-07-22T14:39:33.288709Z",
          "iopub.status.idle": "2025-07-22T14:39:33.301188Z",
          "shell.execute_reply": "2025-07-22T14:39:33.300663Z"
        },
        "papermill": {
          "duration": 0.017581,
          "end_time": "2025-07-22T14:39:33.302272",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.284691",
          "status": "completed"
        },
        "tags": [],
        "id": "f2b6e2a9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#从原始加速度信号中去掉重力分量，得到线性加速度\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "    if isinstance(acc_data, pd.DataFrame):\n",
        "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    else:\n",
        "        acc_values = acc_data\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "    num_samples = acc_values.shape[0]\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "    for i in range(num_samples):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "             linear_accel[i, :] = acc_values[i, :]\n",
        "    return linear_accel\n",
        "#从连续四元数计算角速度 𝜔\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_vel = np.zeros((num_samples, 3))\n",
        "    for i in range(num_samples - 1):\n",
        "        q_t = quat_values[i]\n",
        "        q_t_plus_dt = quat_values[i+1]\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return angular_vel\n",
        "#计算每个时间步的旋转量大小（角位移/旋转幅度）\n",
        "\n",
        "def calculate_angular_distance(rot_data):\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_dist = np.zeros(num_samples)\n",
        "    for i in range(num_samples - 1):\n",
        "        q1 = quat_values[i]\n",
        "        q2 = quat_values[i+1]\n",
        "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
        "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
        "            angular_dist[i] = 0\n",
        "            continue\n",
        "        try:\n",
        "            r1 = R.from_quat(q1)\n",
        "            r2 = R.from_quat(q2)\n",
        "            relative_rotation = r1.inv() * r2\n",
        "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
        "            angular_dist[i] = angle\n",
        "        except ValueError:\n",
        "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
        "            pass\n",
        "    return angular_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CMIFeDataset：负责 单序列的特征提取 + 标准化 + padding\n",
        "\n",
        "CMIFoldDataset：负责 K-fold 划分 + fold 统计\n",
        "\n",
        "**特征来源：**\n",
        "\n",
        "IMU → 加速度、角速度、去重力加速度\n",
        "\n",
        "THM → 热成像像素\n",
        "\n",
        "ToF → 距离像素统计\n",
        "\n",
        "支持： **加粗文字**\n",
        "\n",
        "*缺失值填充：* 有些传感器可能会缺失数据（NaN），模型无法直接处理 NaN，需要填充\n",
        "\n",
        "def get_nan_value(self, data, ratio):\n",
        "\n",
        "    max_value = data.max().max()   # 找到该特征矩阵中的最大值\n",
        "\n",
        "    nan_value = -max_value * ratio # 取负数比例\n",
        "\n",
        "    return nan_value\n",
        "为什么用负比例：\n",
        "\n",
        "保证填充值远离正常数据范围（不会和真实数据混淆），可以让模型学习到这是“缺失值”，而不是正常值，其中ratio是超参数，不用精确，只需要能和正常值区分即可\n",
        "\n",
        "*one-hot 标签：*分类模型需要每个样本对应一个向量表示类别，而不是单个整数。适合多分类交叉熵损失（CrossEntropyLoss）\n",
        "推理/训练使用：输出 logits → 和 one-hot 标签做损失计算\n",
        "\n",
        "*类别权重：*处理类别不平衡问题（少数类不会被忽略）。compute_class_weight('balanced', ...) 根据每类样本数自动计算权重：weight=N/（i*C）其中，N：总样本数；i：类别 i 样本数；C：类别数\n",
        "\n",
        "*推理时单序列处理：*在训练时处理整个数据集，而推理时通常只有 单条序列，需要同样的特征计算、标准化、padding。"
      ],
      "metadata": {
        "id": "3zJV2tiawKnZ"
      },
      "id": "3zJV2tiawKnZ"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "14c3cd30",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2025-07-22T14:39:33.310201Z",
          "iopub.status.busy": "2025-07-22T14:39:33.309974Z",
          "iopub.status.idle": "2025-07-22T14:39:33.360961Z",
          "shell.execute_reply": "2025-07-22T14:39:33.360440Z"
        },
        "papermill": {
          "duration": 0.056741,
          "end_time": "2025-07-22T14:39:33.362250",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.305509",
          "status": "completed"
        },
        "tags": [],
        "id": "14c3cd30"
      },
      "outputs": [],
      "source": [
        "class CMIFeDataset(Dataset):\n",
        "    def __init__(self, data_path, config):\n",
        "        self.config = config\n",
        "        self.init_feature_names(data_path)\n",
        "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols+self.feature_cols)))\n",
        "        self.generate_dataset(df)\n",
        "\n",
        "    def init_feature_names(self, data_path):\n",
        "        self.imu_engineered_features = [\n",
        "            'acc_mag', 'rot_angle',\n",
        "            'acc_mag_jerk', 'rot_angle_vel',\n",
        "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
        "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
        "            'angular_distance'\n",
        "        ]\n",
        "\n",
        "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
        "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
        "        self.tof_cols = self.generate_tof_feature_names()\n",
        "\n",
        "        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
        "        imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
        "        imu_cols_base.extend([c for c in columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
        "        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n",
        "        self.thm_cols = [c for c in columns if c.startswith('thm_')]\n",
        "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
        "        self.imu_dim = len(self.imu_cols)\n",
        "        self.thm_dim = len(self.thm_cols)\n",
        "        self.tof_dim = len(self.tof_cols)\n",
        "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
        "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
        "                          'sequence_id', 'subject',\n",
        "                          'sequence_type', 'gesture', 'orientation'] + [c for c in columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
        "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation']\n",
        "\n",
        "    def generate_tof_feature_names(self):\n",
        "        features = []\n",
        "        if self.config.get(\"tof_raw\", False):\n",
        "            for i in range(1, 6):\n",
        "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
        "        for i in range(1, 6):\n",
        "            if self.tof_mode != 0:\n",
        "                for stat in self.tof_region_stats:\n",
        "                    features.append(f'tof_{i}_{stat}')\n",
        "                if self.tof_mode > 1:\n",
        "                    for r in range(self.tof_mode):\n",
        "                        for stat in self.tof_region_stats:\n",
        "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
        "                if self.tof_mode == -1:\n",
        "                    for mode in [2, 4, 8, 16, 32]:\n",
        "                        for r in range(mode):\n",
        "                            for stat in self.tof_region_stats:\n",
        "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
        "        return features\n",
        "\n",
        "    def compute_features(self, df):\n",
        "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
        "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
        "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
        "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
        "\n",
        "        linear_accel_list = []\n",
        "        for _, group in df.groupby('sequence_id'):\n",
        "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
        "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
        "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
        "        df_linear_accel = pd.concat(linear_accel_list)\n",
        "        df = pd.concat([df, df_linear_accel], axis=1)\n",
        "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
        "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
        "\n",
        "        angular_vel_list = []\n",
        "        for _, group in df.groupby('sequence_id'):\n",
        "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
        "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
        "        df_angular_vel = pd.concat(angular_vel_list)\n",
        "        df = pd.concat([df, df_angular_vel], axis=1)\n",
        "\n",
        "        angular_distance_list = []\n",
        "        for _, group in df.groupby('sequence_id'):\n",
        "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
        "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
        "        df_angular_distance = pd.concat(angular_distance_list)\n",
        "        df = pd.concat([df, df_angular_distance], axis=1)\n",
        "\n",
        "        if self.tof_mode != 0:\n",
        "            new_columns = {}\n",
        "            for i in range(1, 6):\n",
        "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
        "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
        "                new_columns.update({\n",
        "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
        "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
        "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
        "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
        "                })\n",
        "                if self.tof_mode > 1:\n",
        "                    region_size = 64 // self.tof_mode\n",
        "                    for r in range(self.tof_mode):\n",
        "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
        "                        new_columns.update({\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
        "                        })\n",
        "                if self.tof_mode == -1:\n",
        "                    for mode in [2, 4, 8, 16, 32]:\n",
        "                        region_size = 64 // mode\n",
        "                        for r in range(mode):\n",
        "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
        "                            new_columns.update({\n",
        "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
        "                            })\n",
        "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
        "        return df\n",
        "\n",
        "    def generate_features(self, df):\n",
        "        self.le = LabelEncoder()\n",
        "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
        "        self.class_num = len(self.le.classes_)\n",
        "\n",
        "        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n",
        "            print(\"Have precomputed, skip compute.\")\n",
        "        else:\n",
        "            print(\"Not precomputed, do compute.\")\n",
        "            df = self.compute_features(df)\n",
        "\n",
        "        if self.config.get(\"save_precompute\", False):\n",
        "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
        "        return df\n",
        "\n",
        "    def scale(self, data_unscaled):\n",
        "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
        "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
        "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
        "\n",
        "    def pad(self, data_scaled, cols):\n",
        "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
        "        for i, seq in enumerate(data_scaled):\n",
        "            seq_len = min(len(seq), self.pad_len)\n",
        "            pad_data[i, :seq_len] = seq[:seq_len]\n",
        "        return pad_data\n",
        "\n",
        "    def get_nan_value(self, data, ratio):\n",
        "        max_value = data.max().max()\n",
        "        nan_value = -max_value * ratio\n",
        "        return nan_value\n",
        "\n",
        "    def generate_dataset(self, df):\n",
        "        seq_gp = df.groupby('sequence_id')\n",
        "        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n",
        "        classes, lens = [], []\n",
        "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
        "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
        "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
        "\n",
        "        self.fold_feats = defaultdict(list)\n",
        "        for seq_id, seq_df in seq_gp:\n",
        "            imu_data = seq_df[self.imu_cols]\n",
        "            if self.config[\"fbfill\"][\"imu\"]:\n",
        "                imu_data = imu_data.ffill().bfill()\n",
        "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
        "\n",
        "            thm_data = seq_df[self.thm_cols]\n",
        "            if self.config[\"fbfill\"][\"thm\"]:\n",
        "                thm_data = thm_data.ffill().bfill()\n",
        "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
        "\n",
        "            tof_data = seq_df[self.tof_cols]\n",
        "            if self.config[\"fbfill\"][\"tof\"]:\n",
        "                tof_data = tof_data.ffill().bfill()\n",
        "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
        "\n",
        "            classes.append(seq_df['gesture_int'].iloc[0])\n",
        "            lens.append(len(imu_data))\n",
        "\n",
        "            for col in self.fold_cols:\n",
        "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
        "\n",
        "        self.dataset_indices = classes\n",
        "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
        "        if self.config.get(\"one_scale\", True):\n",
        "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
        "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
        "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
        "            self.imu = x[..., :self.imu_dim]\n",
        "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
        "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
        "        else:\n",
        "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
        "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
        "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
        "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
        "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
        "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
        "        self.precompute_scaled_nan_values()\n",
        "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
        "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
        "\n",
        "    def precompute_scaled_nan_values(self):\n",
        "        dummy_df = pd.DataFrame(\n",
        "            np.array([[self.imu_nan_value]*len(self.imu_cols) +\n",
        "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
        "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
        "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
        "        )\n",
        "\n",
        "        if self.config.get(\"one_scale\", True):\n",
        "            scaled = self.x_scaler.transform(dummy_df)\n",
        "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
        "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
        "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
        "        else:\n",
        "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
        "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
        "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
        "\n",
        "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
        "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
        "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
        "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
        "\n",
        "    def inference_process(self, sequence):\n",
        "        df_seq = sequence.to_pandas().copy()\n",
        "        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n",
        "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
        "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
        "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
        "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
        "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
        "                linear_accel = remove_gravity_from_acc(\n",
        "                    df_seq[['acc_x', 'acc_y', 'acc_z']],\n",
        "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "                )\n",
        "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
        "            else:\n",
        "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
        "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
        "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
        "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
        "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
        "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
        "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
        "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
        "            else:\n",
        "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
        "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
        "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
        "            else:\n",
        "                df_seq['angular_distance'] = 0\n",
        "\n",
        "        if self.tof_mode != 0:\n",
        "            new_columns = {}\n",
        "            for i in range(1, 6):\n",
        "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
        "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
        "                new_columns.update({\n",
        "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
        "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
        "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
        "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
        "                })\n",
        "                if self.tof_mode > 1:\n",
        "                    region_size = 64 // self.tof_mode\n",
        "                    for r in range(self.tof_mode):\n",
        "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
        "                        new_columns.update({\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
        "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
        "                        })\n",
        "                if self.tof_mode == -1:\n",
        "                    for mode in [2, 4, 8, 16, 32]:\n",
        "                        region_size = 64 // mode\n",
        "                        for r in range(mode):\n",
        "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
        "                            new_columns.update({\n",
        "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
        "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
        "                            })\n",
        "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
        "\n",
        "        imu_unscaled = df_seq[self.imu_cols]\n",
        "        if self.config[\"fbfill\"][\"imu\"]:\n",
        "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
        "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
        "\n",
        "        thm_unscaled = df_seq[self.thm_cols]\n",
        "        if self.config[\"fbfill\"][\"thm\"]:\n",
        "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
        "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
        "\n",
        "        tof_unscaled = df_seq[self.tof_cols]\n",
        "        if self.config[\"fbfill\"][\"tof\"]:\n",
        "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
        "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
        "\n",
        "        if self.config.get(\"one_scale\", True):\n",
        "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
        "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
        "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
        "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
        "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
        "        else:\n",
        "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
        "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
        "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
        "\n",
        "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
        "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
        "        seq_len = min(combined.shape[0], self.pad_len)\n",
        "        padded[:seq_len] = combined[:seq_len]\n",
        "        imu = padded[..., :self.imu_dim]\n",
        "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
        "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
        "\n",
        "        return torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_)\n",
        "\n",
        "class CMIFoldDataset:\n",
        "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
        "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
        "        self.imu_dim = self.full_dataset.imu_dim\n",
        "        self.thm_dim = self.full_dataset.thm_dim\n",
        "        self.tof_dim = self.full_dataset.tof_dim\n",
        "        self.le = self.full_dataset.le\n",
        "        self.class_names = self.full_dataset.le.classes_\n",
        "        self.class_weight = self.full_dataset.class_weight\n",
        "        self.n_folds = n_folds\n",
        "        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
        "        self.folds = list(self.skf.split(np.arange(len(self.full_dataset)), np.array(self.full_dataset.dataset_indices)))\n",
        "\n",
        "    def get_fold_datasets(self, fold_idx):\n",
        "        if self.folds is None or fold_idx >= self.n_folds:\n",
        "            return None, None\n",
        "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
        "        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n",
        "\n",
        "    def print_fold_stats(self):\n",
        "        def get_label_counts(subset):\n",
        "            counts = {name: 0 for name in self.class_names}\n",
        "            if subset is None:\n",
        "                return counts\n",
        "            for idx in subset.indices:\n",
        "                label_idx = self.full_dataset.dataset_indices[idx]\n",
        "                counts[self.class_names[label_idx]] += 1\n",
        "            return counts\n",
        "\n",
        "        print(\"\\n交叉验证折叠统计:\")\n",
        "        for fold_idx in range(self.n_folds):\n",
        "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
        "            train_counts = get_label_counts(train_fold)\n",
        "            valid_counts = get_label_counts(valid_fold)\n",
        "\n",
        "            print(f\"\\nFold {fold_idx + 1}:\")\n",
        "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
        "            for name in self.class_names:\n",
        "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2f5858",
      "metadata": {
        "papermill": {
          "duration": 0.003262,
          "end_time": "2025-07-22T14:39:33.369138",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.365876",
          "status": "completed"
        },
        "tags": [],
        "id": "af2f5858"
      },
      "source": [
        "# Model\n",
        "\n",
        "Use bert instead of simple attention layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ba6ddb",
      "metadata": {
        "id": "03ba6ddb"
      },
      "source": [
        "⚠️ **此单元已禁用：替换为多模态 Transformer 版本。原始代码保留供参考。**\n",
        "\n",
        "```python\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction = 8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
        "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
        "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
        "        return x * se                \n",
        "\n",
        "class ResNetSEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        # SE\n",
        "        self.se = SEBlock(out_channels)\n",
        "        \n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
        "                          padding=0, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x) :\n",
        "        identity = self.shortcut(x)              # (B, out, L)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)                       # (B, out, L)\n",
        "        out = out + identity\n",
        "        return self.relu(out)\n",
        "\n",
        "class CMIModel(nn.Module):\n",
        "    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n",
        "        super().__init__()\n",
        "        self.imu_branch = nn.Sequential(\n",
        "            self.residual_se_cnn_block(imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"],\n",
        "                                       drop=kwargs[\"imu1_dropout\"]),\n",
        "            self.residual_se_cnn_block(kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"],\n",
        "                                       drop=kwargs[\"imu2_dropout\"])\n",
        "        )\n",
        "\n",
        "        self.thm_branch = nn.Sequential(\n",
        "            nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"thm1_channels\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(2, ceil_mode=True),\n",
        "            nn.Dropout(kwargs[\"thm1_dropout\"]),\n",
        "            \n",
        "            nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(2, ceil_mode=True),\n",
        "            nn.Dropout(kwargs[\"thm2_dropout\"])\n",
        "        )\n",
        "        \n",
        "        self.tof_branch = nn.Sequential(\n",
        "            nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"tof1_channels\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(2, ceil_mode=True),\n",
        "            nn.Dropout(kwargs[\"tof1_dropout\"]),\n",
        "            \n",
        "            nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(2, ceil_mode=True),\n",
        "            nn.Dropout(kwargs[\"tof2_dropout\"])\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n",
        "        self.bert = BertModel(BertConfig(\n",
        "            hidden_size=kwargs[\"feat_dim\"],\n",
        "            num_hidden_layers=kwargs[\"bert_layers\"],\n",
        "            num_attention_heads=kwargs[\"bert_heads\"],\n",
        "            intermediate_size=kwargs[\"feat_dim\"]*4\n",
        "        ))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"cls1_channels\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(kwargs[\"cls1_dropout\"]),\n",
        "            nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False),\n",
        "            nn.BatchNorm1d(kwargs[\"cls2_channels\"]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(kwargs[\"cls2_dropout\"]),\n",
        "            nn.Linear(kwargs[\"cls2_channels\"], n_classes)\n",
        "        )\n",
        "    \n",
        "    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n",
        "        return nn.Sequential(\n",
        "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
        "            ResNetSEBlock(in_channels, out_channels, wd=wd),\n",
        "            nn.MaxPool1d(pool_size),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "    \n",
        "    def forward(self, imu, thm, tof):\n",
        "        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n",
        "        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n",
        "        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n",
        "        \n",
        "        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n",
        "        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)  # (B,1,H)\n",
        "        bert_input = torch.cat([cls_token, bert_input], dim=1)  # (B,T+1,H)\n",
        "        outputs = self.bert(inputs_embeds=bert_input)\n",
        "        pred_cls = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return self.classifier(pred_cls)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ed5da3",
      "metadata": {
        "papermill": {
          "duration": 0.003149,
          "end_time": "2025-07-22T14:39:33.400634",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.397485",
          "status": "completed"
        },
        "tags": [],
        "id": "c9ed5da3"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e7cd7f6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:39:33.407782Z",
          "iopub.status.busy": "2025-07-22T14:39:33.407391Z",
          "iopub.status.idle": "2025-07-22T14:39:33.914912Z",
          "shell.execute_reply": "2025-07-22T14:39:33.914246Z"
        },
        "papermill": {
          "duration": 0.512513,
          "end_time": "2025-07-22T14:39:33.916275",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.403762",
          "status": "completed"
        },
        "tags": [],
        "id": "e7cd7f6d"
      },
      "outputs": [],
      "source": [
        "CUDA0 = \"cuda:0\"\n",
        "seed = 0\n",
        "batch_size = 64\n",
        "num_workers = 4\n",
        "n_folds = 5\n",
        "\n",
        "#universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
        "universe_csv_path = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
        "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
        "deterministic.init_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "30f585d0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:39:33.924306Z",
          "iopub.status.busy": "2025-07-22T14:39:33.924084Z",
          "iopub.status.idle": "2025-07-22T14:41:55.528153Z",
          "shell.execute_reply": "2025-07-22T14:41:55.527326Z"
        },
        "papermill": {
          "duration": 141.61317,
          "end_time": "2025-07-22T14:41:55.533258",
          "exception": false,
          "start_time": "2025-07-22T14:39:33.920088",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "30f585d0",
        "outputId": "626c5dd3-57cf-46fb-9415-3e8d15ebb14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model files: /kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\n",
            "['fold1', 'fold3', 'fold0', 'fold4', 'fold2']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2459035629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2459035629.py\u001b[0m in \u001b[0;36minit_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m\"save_precompute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     }\n\u001b[0;32m---> 30\u001b[0;31m     dataset = CMIFoldDataset(universe_csv_path, dataset_config,\n\u001b[0m\u001b[1;32m     31\u001b[0m                              n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n\u001b[1;32m     32\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_fold_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3414564624.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, config, full_dataset_function, n_folds, random_seed)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCMIFoldDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_dataset_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimu_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimu_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthm_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthm_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3414564624.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, config)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3414564624.py\u001b[0m in \u001b[0;36minit_feature_names\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtof_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_tof_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mimu_cols_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear_acc_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_acc_y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_acc_z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimu_cols_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rot_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'rot_angle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rot_angle_vel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1'"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(\"wasupandceacar/cmi-models-public/pyTorch/train_fold_model05_tof16_raw\")\n",
        "\n",
        "print(\"Path to model files:\", path)\n",
        "import os\n",
        "path = \"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\"\n",
        "print(os.listdir(path))\n",
        "\n",
        "def init_dataset():\n",
        "    dataset_config = {\n",
        "        \"percent\": 95,\n",
        "        \"scaler_function\": StandardScaler(),\n",
        "        \"nan_ratio\": {\n",
        "            \"imu\": 0,\n",
        "            \"thm\": 0,\n",
        "            \"tof\": 0,\n",
        "        },\n",
        "        \"fbfill\": {\n",
        "            \"imu\": True,\n",
        "            \"thm\": True,\n",
        "            \"tof\": True,\n",
        "        },\n",
        "        \"one_scale\": True,\n",
        "        \"tof_raw\": True,\n",
        "        \"tof_mode\": 16,\n",
        "        \"save_precompute\": False,\n",
        "    }\n",
        "    dataset = CMIFoldDataset(universe_csv_path, dataset_config,\n",
        "                             n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n",
        "    dataset.print_fold_stats()\n",
        "    return dataset\n",
        "\n",
        "def get_fold_dataset(dataset, fold):\n",
        "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "    return valid_loader\n",
        "\n",
        "dataset = init_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85faf7a9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:41:55.541164Z",
          "iopub.status.busy": "2025-07-22T14:41:55.540947Z",
          "iopub.status.idle": "2025-07-22T14:42:04.187372Z",
          "shell.execute_reply": "2025-07-22T14:42:04.186557Z"
        },
        "papermill": {
          "duration": 8.651876,
          "end_time": "2025-07-22T14:42:04.188771",
          "exception": false,
          "start_time": "2025-07-22T14:41:55.536895",
          "status": "completed"
        },
        "tags": [],
        "id": "85faf7a9"
      },
      "outputs": [],
      "source": [
        "model_function = CMIModel\n",
        "model_args = {\"feat_dim\": 500,\n",
        "              \"imu1_channels\": 219, \"imu1_dropout\": 0.2946731587132302, \"imu2_dropout\": 0.2697745571929592,\n",
        "              \"imu1_weight_decay\": 0.0014824054650601245, \"imu2_weight_decay\": 0.002742543773142381,\n",
        "              \"imu1_layers\": 0, \"imu2_layers\": 0,\n",
        "              \"thm1_channels\": 82, \"thm1_dropout\": 0.2641274454844602, \"thm2_dropout\": 0.302896343020985,\n",
        "              \"tof1_channels\": 82, \"tof1_dropout\": 0.2641274454844602, \"tof2_dropout\": 0.3028963430209852,\n",
        "              \"bert_layers\": 8, \"bert_heads\": 10,\n",
        "              \"cls1_channels\": 937, \"cls2_channels\": 303, \"cls1_dropout\": 0.2281834512100508, \"cls2_dropout\": 0.22502521933558461}\n",
        "model_args.update({\n",
        "    \"imu_dim\": dataset.full_dataset.imu_dim,\n",
        "    \"thm_dim\": dataset.full_dataset.thm_dim,\n",
        "    \"tof_dim\": dataset.full_dataset.tof_dim,\n",
        "    \"n_classes\": dataset.full_dataset.class_num})\n",
        "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
        "\n",
        "model_dicts = [\n",
        "    {\n",
        "        \"model_function\": model_function,\n",
        "        \"model_args\": model_args,\n",
        "        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n",
        "    } for fold in range(n_folds)\n",
        "]\n",
        "\n",
        "models = list()\n",
        "for model_dict in model_dicts:\n",
        "    model_function = model_dict[\"model_function\"]\n",
        "    model_args = model_dict[\"model_args\"]\n",
        "    model_path = model_dict[\"model_path\"]\n",
        "    model = model_function(**model_args).to(CUDA0)\n",
        "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in torch.load(model_path).items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3327fca",
      "metadata": {
        "papermill": {
          "duration": 0.003563,
          "end_time": "2025-07-22T14:42:04.195900",
          "exception": false,
          "start_time": "2025-07-22T14:42:04.192337",
          "status": "completed"
        },
        "tags": [],
        "id": "c3327fca"
      },
      "source": [
        "# Valid\n",
        "\n",
        "According to competition test datasets, valid both on full data and Imu only data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62f8e11",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:42:04.203311Z",
          "iopub.status.busy": "2025-07-22T14:42:04.203073Z",
          "iopub.status.idle": "2025-07-22T14:43:03.430226Z",
          "shell.execute_reply": "2025-07-22T14:43:03.428916Z"
        },
        "papermill": {
          "duration": 59.232612,
          "end_time": "2025-07-22T14:43:03.431858",
          "exception": false,
          "start_time": "2025-07-22T14:42:04.199246",
          "status": "completed"
        },
        "tags": [],
        "id": "f62f8e11"
      },
      "outputs": [],
      "source": [
        "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n",
        "\n",
        "metric = metric_package.Metric()\n",
        "imu_only_metric = metric_package.Metric()\n",
        "\n",
        "def to_cuda(*tensors):\n",
        "    return [tensor.to(CUDA0) for tensor in tensors]\n",
        "\n",
        "def predict_valid(model, imu, thm, tof):\n",
        "    pred = model(imu, thm, tof)\n",
        "    return pred\n",
        "\n",
        "def valid(model, valid_bar):\n",
        "    with torch.no_grad():\n",
        "        for imu, thm, tof, y in valid_bar:\n",
        "            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n",
        "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                logits = predict_valid(model, imu, thm, tof)\n",
        "            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
        "            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
        "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                logits = model(imu, thm, tof)\n",
        "            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
        "\n",
        "for fold, model in enumerate(models):\n",
        "    valid_loader = get_fold_dataset(dataset, fold)\n",
        "    valid_bar = tqdm(valid_loader, desc=f\"Valid\", position=0, leave=False)\n",
        "    valid(model, valid_bar)\n",
        "\n",
        "print(f\"\"\"\n",
        "Normal score: {metric.score()}\n",
        "IMU only score: {imu_only_metric.score()}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787a5956",
      "metadata": {
        "papermill": {
          "duration": 0.004152,
          "end_time": "2025-07-22T14:43:03.441050",
          "exception": false,
          "start_time": "2025-07-22T14:43:03.436898",
          "status": "completed"
        },
        "tags": [],
        "id": "787a5956"
      },
      "source": [
        "# Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f742d3b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:43:03.451476Z",
          "iopub.status.busy": "2025-07-22T14:43:03.451212Z",
          "iopub.status.idle": "2025-07-22T14:43:05.522042Z",
          "shell.execute_reply": "2025-07-22T14:43:05.521318Z"
        },
        "papermill": {
          "duration": 2.077261,
          "end_time": "2025-07-22T14:43:05.523414",
          "exception": false,
          "start_time": "2025-07-22T14:43:03.446153",
          "status": "completed"
        },
        "tags": [],
        "id": "f742d3b8"
      },
      "outputs": [],
      "source": [
        "def avg_predict(models, imu, thm, tof):\n",
        "    outputs = []\n",
        "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "        for model in models:\n",
        "            logits = model(imu, thm, tof)\n",
        "            outputs.append(logits)\n",
        "    return torch.mean(torch.stack(outputs), dim=0)\n",
        "\n",
        "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
        "    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n",
        "    with torch.no_grad():\n",
        "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
        "        logits = avg_predict(models, imu, thm, tof)\n",
        "    return dataset.le.classes_[logits.argmax(dim=1).cpu()]\n",
        "\n",
        "import kaggle_evaluation.cmi_inference_server\n",
        "\n",
        "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(\n",
        "        data_paths=(\n",
        "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
        "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
        "        )\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 12518947,
          "isSourceIdPinned": false,
          "sourceId": 102335,
          "sourceType": "competition"
        },
        {
          "sourceId": 240649816,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 249959489,
          "sourceType": "kernelVersion"
        },
        {
          "isSourceIdPinned": false,
          "modelId": 398856,
          "modelInstanceId": 379625,
          "sourceId": 470587,
          "sourceType": "modelInstanceVersion"
        },
        {
          "isSourceIdPinned": false,
          "modelId": 400086,
          "modelInstanceId": 380358,
          "sourceId": 471764,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 253.578364,
      "end_time": "2025-07-22T14:43:09.793664",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-07-22T14:38:56.215300",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55c940c1",
        "db6488f2",
        "af2f5858"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}